"""
Wrapper script to run query_codebase.py from within the graph_rag package.
Sets up the Python path correctly for local imports.
"""
import sys
import os
import argparse

# Add parent directory to path so 'graph_rag' package can be found
parent_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
if parent_dir not in sys.path:
    sys.path.insert(0, parent_dir)

# Also ensure current directory is in path for direct imports
current_dir = os.path.dirname(os.path.abspath(__file__))
if current_dir not in sys.path:
    sys.path.insert(0, current_dir)

# Set API keys
os.environ["VOYAGE_API_KEY"] = "pa-72D-aiLWPIFDM0jj3cbUP_de843HzPXVKGPnP5sYbnQ"
os.environ["GOOGLE_API_KEY"] = "AIzaSyDZGk1eTArJrWpMx6FiKUZpXUrGIL9zs9w"

from datetime import datetime
from pipeline import GraphRAGPipeline
from models import EnrichedChunk
from exceptions import IndexNotFoundError, RetrievalError, GenerationError


def generate_markdown_report(
    query: str,
    search_results: list,
    reranked_results: list,
    expanded_results: list,
    llm_response: str,
    output_path: str,
    pipeline: GraphRAGPipeline
):
    """Generate a detailed markdown report of the query results."""
    
    timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
    
    md_content = f"""# Graph-Enhanced RAG Query Report

**Generated:** {timestamp}

---

## Query

```
{query}
```

---

## Summary

| Metric | Value |
|--------|-------|
| Vector Search Results | {len(search_results)} |
| Reranked Results | {len(reranked_results)} |
| Graph-Expanded Context | {len(expanded_results)} |
| Total Context Chunks | {len(reranked_results) + len(expanded_results)} |

---

## LLM Response

{llm_response}

---

## Detailed Results

### Primary Matches (Vector Search + Reranking)

These are the most relevant code chunks found via semantic search and reranked for relevance.

"""
    
    for i, (doc, result) in enumerate(zip(reranked_results, search_results), 1):
        # Parse the enriched chunk text to extract metadata
        chunk = EnrichedChunk.from_text(doc)
        
        md_content += f"""#### [{i}] {chunk.function_name}

**File:** `{chunk.file_path}`

**Node ID:** `{chunk.node_id}`

**Distance:** {result.distance:.4f}

**Context:**
- Defined In: `{chunk.defined_in or 'N/A'}`
- Calls: {', '.join(f'`{c}`' for c in chunk.calls) if chunk.calls else 'None'}
- Called By: {', '.join(f'`{c}`' for c in chunk.called_by) if chunk.called_by else 'None'}

**Code:**
```python
{chunk.code}
```

---

"""
    
    md_content += """### Graph-Expanded Context (Related Code)

These are related code chunks discovered by traversing the knowledge graph (CALLS/CALLED_BY relationships).

"""
    
    if expanded_results:
        for i, content in enumerate(expanded_results, 1):
            # Try to parse as EnrichedChunk
            try:
                chunk = EnrichedChunk.from_text(content)
                md_content += f"""#### [Dep {i}] {chunk.function_name}

**File:** `{chunk.file_path}`

**Relationship:** Graph neighbor (CALLS/CALLED_BY)

**Context:**
- Defined In: `{chunk.defined_in or 'N/A'}`
- Calls: {', '.join(f'`{c}`' for c in chunk.calls) if chunk.calls else 'None'}
- Called By: {', '.join(f'`{c}`' for c in chunk.called_by) if chunk.called_by else 'None'}

**Code:**
```python
{chunk.code}
```

---

"""
            except Exception:
                # Fallback for raw content
                md_content += f"""#### [Dep {i}] Related Code

```
{content[:500]}{'...' if len(content) > 500 else ''}
```

---

"""
    else:
        md_content += "*No graph-expanded context found.*\n\n---\n\n"
    
    md_content += f"""## Raw Data

### Search Results Node IDs

```
{chr(10).join(r.node_id for r in search_results)}
```

### Graph Statistics

- Total Graph Nodes: {pipeline.graph.number_of_nodes() if pipeline.graph else 'N/A'}
- Total Graph Edges: {pipeline.graph.number_of_edges() if pipeline.graph else 'N/A'}
- Indexed Functions: {len(pipeline.master_list) if pipeline.master_list else 'N/A'}
- Enriched Chunks: {len(pipeline.enriched_chunks) if pipeline.enriched_chunks else 'N/A'}

---

*Report generated by Graph-Enhanced RAG System*
"""
    
    # Write to file
    with open(output_path, 'w', encoding='utf-8') as f:
        f.write(md_content)
    
    return md_content


def run_query(query: str, output_file: str = "query_result.md", db_path: str = "./lancedb_store", data_dir: str = "./rag_data"):
    """Run a single query and generate a report."""
    
    print("=" * 60)
    print("GRAPH-ENHANCED RAG - QUERY PHASE")
    print("=" * 60)
    
    # Initialize pipeline
    pipeline = GraphRAGPipeline(
        db_path=db_path,
        data_dir=data_dir
    )
    
    # Check if index exists
    if not pipeline.is_indexed():
        print("\n[ERROR] No index found!")
        print("  Please run 'python run_index.py' first.")
        return False
    
    # Load index
    try:
        pipeline._ensure_index_loaded()
        print(f"\n[SUCCESS] Index loaded successfully")
        print(f"  - Graph nodes: {pipeline.graph.number_of_nodes()}")
        print(f"  - Functions: {len(pipeline.master_list)}")
        print(f"  - Chunks: {len(pipeline.enriched_chunks)}")
    except Exception as e:
        print(f"\n[ERROR] Failed to load index: {e}")
        return False
    
    print("\n" + "-" * 60)
    print(f"Query: {query}")
    print("-" * 60)
    
    top_k = 5
    rerank_top_k = 3
    
    try:
        # Step 1: Embed query
        print("\n[1/6] Embedding query...")
        query_embedding = pipeline.embedding_service.embed_query(query)
        
        # Step 2: Vector search
        print("[2/6] Searching vector database...")
        search_results = pipeline.vector_store.search(query_embedding, limit=top_k * 2)
        print(f"      Found {len(search_results)} initial results")
        
        # Step 3: Rerank
        print("[3/6] Reranking results...")
        documents = [r.text for r in search_results]
        reranked = pipeline.embedding_service.rerank(query, documents, top_k=rerank_top_k)
        reranked_docs = [r.document for r in reranked]
        reranked_node_ids = [search_results[r.index].node_id for r in reranked]
        print(f"      Reranked to {len(reranked_docs)} results")
        
        # Step 4: Graph expansion
        print("[4/6] Expanding via knowledge graph...")
        expanded_results = pipeline._expand_results(reranked_node_ids, set(reranked_node_ids))
        expanded_context = [r.content for r in expanded_results]
        print(f"      Found {len(expanded_context)} related chunks")
        
        # Step 5: Assemble context
        print("[5/6] Assembling context...")
        prompt = pipeline.context_assembler.assemble(
            primary_results=reranked_docs,
            secondary_results=expanded_context,
            query=query
        )
        print(f"      Prompt length: {len(prompt)} chars")
        
        # Step 6: Generate response
        print("[6/6] Generating LLM response...")
        llm_response = pipeline.llm_service.generate(prompt, stream=False)
        print(f"      Response length: {len(llm_response)} chars")
        
        # Generate markdown report
        print(f"\nGenerating report: {output_file}")
        generate_markdown_report(
            query=query,
            search_results=search_results[:rerank_top_k],
            reranked_results=reranked_docs,
            expanded_results=expanded_context,
            llm_response=llm_response,
            output_path=output_file,
            pipeline=pipeline
        )
        
        print("\n" + "=" * 60)
        print("[SUCCESS] QUERY COMPLETED SUCCESSFULLY")
        print("=" * 60)
        print(f"\nResults saved to: {output_file}")
        print(f"\n--- LLM Response Preview ---\n")
        print(llm_response[:800])
        if len(llm_response) > 800:
            print(f"\n... (see {output_file} for full response)")
        
        return True
        
    except Exception as e:
        print(f"\n[ERROR] Query failed: {e}")
        import traceback
        traceback.print_exc()
        return False


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Query an indexed codebase")
    parser.add_argument(
        "query",
        nargs="?",
        default="How does the call resolution work?",
        help="Query to run"
    )
    parser.add_argument(
        "--output", "-o",
        default="query_result.md",
        help="Output markdown file path"
    )
    parser.add_argument(
        "--db-path",
        default="./lancedb_store",
        help="Path to LanceDB database directory"
    )
    parser.add_argument(
        "--data-dir",
        default="./rag_data",
        help="Path to graph and metadata files"
    )
    
    args = parser.parse_args()
    
    success = run_query(args.query, args.output, args.db_path, args.data_dir)
    sys.exit(0 if success else 1)
