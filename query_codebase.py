"""
Query Codebase - Phase 2: Query the Indexed Database

This script queries an indexed codebase and outputs detailed results to a markdown file.
It reads a single question from the terminal and produces comprehensive output including:
- Query details
- Vector search results
- Reranked results
- Graph-expanded context
- Final LLM response

Usage:
    python query_codebase.py
    python query_codebase.py --output results.md
"""

import os
import sys
import argparse
from datetime import datetime

# Set API keys
os.environ["VOYAGE_API_KEY"] = "pa-72D-aiLWPIFDM0jj3cbUP_de843HzPXVKGPnP5sYbnQ"
os.environ["GOOGLE_API_KEY"] = "AIzaSyDZGk1eTArJrWpMx6FiKUZpXUrGIL9zs9w"

from graph_rag import GraphRAGPipeline, EnrichedChunk
from graph_rag.exceptions import (
    IndexNotFoundError, RetrievalError, GenerationError
)


def generate_markdown_report(
    query: str,
    search_results: list,
    reranked_results: list,
    expanded_results: list,
    llm_response: str,
    output_path: str,
    pipeline: GraphRAGPipeline
):
    """Generate a detailed markdown report of the query results."""
    
    timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
    
    md_content = f"""# Graph-Enhanced RAG Query Report

**Generated:** {timestamp}

---

## Query

```
{query}
```

---

## Summary

| Metric | Value |
|--------|-------|
| Vector Search Results | {len(search_results)} |
| Reranked Results | {len(reranked_results)} |
| Graph-Expanded Context | {len(expanded_results)} |
| Total Context Chunks | {len(reranked_results) + len(expanded_results)} |

---

## LLM Response

{llm_response}

---

## Detailed Results

### Primary Matches (Vector Search + Reranking)

These are the most relevant code chunks found via semantic search and reranked for relevance.

"""
    
    for i, (doc, result) in enumerate(zip(reranked_results, search_results), 1):
        # Parse the enriched chunk text to extract metadata
        chunk = EnrichedChunk.from_text(doc)
        
        md_content += f"""#### [{i}] {chunk.function_name}

**File:** `{chunk.file_path}`

**Node ID:** `{chunk.node_id}`

**Distance:** {result.distance:.4f}

**Context:**
- Defined In: `{chunk.defined_in or 'N/A'}`
- Calls: {', '.join(f'`{c}`' for c in chunk.calls) if chunk.calls else 'None'}
- Called By: {', '.join(f'`{c}`' for c in chunk.called_by) if chunk.called_by else 'None'}

**Code:**
```python
{chunk.code}
```

---

"""
    
    md_content += """### Graph-Expanded Context (Related Code)

These are related code chunks discovered by traversing the knowledge graph (CALLS/CALLED_BY relationships).

"""
    
    if expanded_results:
        for i, content in enumerate(expanded_results, 1):
            # Try to parse as EnrichedChunk
            try:
                chunk = EnrichedChunk.from_text(content)
                md_content += f"""#### [Dep {i}] {chunk.function_name}

**File:** `{chunk.file_path}`

**Relationship:** Graph neighbor (CALLS/CALLED_BY)

**Context:**
- Defined In: `{chunk.defined_in or 'N/A'}`
- Calls: {', '.join(f'`{c}`' for c in chunk.calls) if chunk.calls else 'None'}
- Called By: {', '.join(f'`{c}`' for c in chunk.called_by) if chunk.called_by else 'None'}

**Code:**
```python
{chunk.code}
```

---

"""
            except Exception:
                # Fallback for raw content
                md_content += f"""#### [Dep {i}] Related Code

```
{content[:500]}{'...' if len(content) > 500 else ''}
```

---

"""
    else:
        md_content += "*No graph-expanded context found.*\n\n---\n\n"
    
    md_content += f"""## Raw Data

### Search Results Node IDs

```
{chr(10).join(r.node_id for r in search_results)}
```

### Graph Statistics

- Total Graph Nodes: {pipeline.graph.number_of_nodes() if pipeline.graph else 'N/A'}
- Total Graph Edges: {pipeline.graph.number_of_edges() if pipeline.graph else 'N/A'}
- Indexed Functions: {len(pipeline.master_list) if pipeline.master_list else 'N/A'}
- Enriched Chunks: {len(pipeline.enriched_chunks) if pipeline.enriched_chunks else 'N/A'}

---

*Report generated by Graph-Enhanced RAG System*
"""
    
    # Write to file
    with open(output_path, 'w', encoding='utf-8') as f:
        f.write(md_content)
    
    return md_content


def main():
    parser = argparse.ArgumentParser(description="Query an indexed codebase")
    parser.add_argument(
        "--db-path",
        default="./lancedb_store",
        help="Path to LanceDB database directory"
    )
    parser.add_argument(
        "--data-dir",
        default="./rag_data",
        help="Path to graph and metadata files"
    )
    parser.add_argument(
        "--output", "-o",
        default="query_result.md",
        help="Output markdown file path"
    )
    parser.add_argument(
        "--top-k",
        type=int,
        default=5,
        help="Number of results to retrieve"
    )
    parser.add_argument(
        "--rerank-top-k",
        type=int,
        default=3,
        help="Number of results after reranking"
    )
    parser.add_argument(
        "--no-expand",
        action="store_true",
        help="Disable graph expansion"
    )
    
    args = parser.parse_args()
    
    print("=" * 60)
    print("GRAPH-ENHANCED RAG - QUERY PHASE")
    print("=" * 60)
    
    # Initialize pipeline
    pipeline = GraphRAGPipeline(
        db_path=args.db_path,
        data_dir=args.data_dir
    )
    
    # Check if index exists
    if not pipeline.is_indexed():
        print("\n[ERROR] No index found!")
        print("  Please run 'python index_codebase.py' first.")
        sys.exit(1)
    
    # Load index
    try:
        pipeline._ensure_index_loaded()
        print(f"\n[SUCCESS] Index loaded successfully")
        print(f"  - Graph nodes: {pipeline.graph.number_of_nodes()}")
        print(f"  - Functions: {len(pipeline.master_list)}")
        print(f"  - Chunks: {len(pipeline.enriched_chunks)}")
    except Exception as e:
        print(f"\n[ERROR] Failed to load index: {e}")
        sys.exit(1)
    
    # Get question from terminal
    print("\n" + "-" * 60)
    query = input("Enter your question: ").strip()
    
    if not query:
        print("[ERROR] No question provided.")
        sys.exit(1)
    
    print("-" * 60)
    print(f"\nProcessing query: {query[:80]}{'...' if len(query) > 80 else ''}")
    
    try:
        # Step 1: Embed query
        print("\n[1/6] Embedding query...")
        query_embedding = pipeline.embedding_service.embed_query(query)
        
        # Step 2: Vector search
        print("[2/6] Searching vector database...")
        search_results = pipeline.vector_store.search(query_embedding, limit=args.top_k * 2)
        print(f"      Found {len(search_results)} initial results")
        
        # Step 3: Rerank
        print("[3/6] Reranking results...")
        documents = [r.text for r in search_results]
        reranked = pipeline.embedding_service.rerank(query, documents, top_k=args.rerank_top_k)
        reranked_docs = [r.document for r in reranked]
        reranked_node_ids = [search_results[r.index].node_id for r in reranked]
        print(f"      Reranked to {len(reranked_docs)} results")
        
        # Step 4: Graph expansion
        expanded_context = []
        if not args.no_expand and reranked_node_ids:
            print("[4/6] Expanding via knowledge graph...")
            expanded_results = pipeline._expand_results(reranked_node_ids, set(reranked_node_ids))
            expanded_context = [r.content for r in expanded_results]
            print(f"      Found {len(expanded_context)} related chunks")
        else:
            print("[4/6] Graph expansion skipped")
        
        # Step 5: Assemble context
        print("[5/6] Assembling context...")
        prompt = pipeline.context_assembler.assemble(
            primary_results=reranked_docs,
            secondary_results=expanded_context,
            query=query
        )
        print(f"      Prompt length: {len(prompt)} chars")
        
        # Step 6: Generate response
        print("[6/6] Generating LLM response...")
        llm_response = pipeline.llm_service.generate(prompt, stream=False)
        print(f"      Response length: {len(llm_response)} chars")
        
        # Generate markdown report
        print(f"\nGenerating report: {args.output}")
        generate_markdown_report(
            query=query,
            search_results=search_results[:args.rerank_top_k],
            reranked_results=reranked_docs,
            expanded_results=expanded_context,
            llm_response=llm_response,
            output_path=args.output,
            pipeline=pipeline
        )
        
        print("\n" + "=" * 60)
        print("[SUCCESS] QUERY COMPLETED SUCCESSFULLY")
        print("=" * 60)
        print(f"\nResults saved to: {args.output}")
        print(f"\n--- LLM Response Preview ---\n")
        print(llm_response[:800])
        if len(llm_response) > 800:
            print(f"\n... (see {args.output} for full response)")
        
    except IndexNotFoundError as e:
        print(f"\n[ERROR] Index not found: {e}")
        sys.exit(1)
    except RetrievalError as e:
        print(f"\n[ERROR] Retrieval failed: {e}")
        sys.exit(1)
    except GenerationError as e:
        print(f"\n[ERROR] LLM generation failed: {e}")
        sys.exit(1)
    except Exception as e:
        print(f"\n[ERROR] Unexpected error: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    main()
